{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from docx import Document\n",
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "     -------------------------------------- 244.3/244.3 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\shria\\anaconda3\\lib\\site-packages (from python-docx) (4.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\shria\\anaconda3\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-1.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shria\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shria\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shria\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shria\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shria\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shria\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shria\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_from_docx(docx_file, output_dir):\n",
    "    document = Document(docx_file)\n",
    "    full_text = []\n",
    "    images = []\n",
    "    tables = []\n",
    "\n",
    "    # Ensure the output directory exists for saving images\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Traverse through document elements\n",
    "    for element in document.element.body:\n",
    "        if element.tag.endswith(\"p\"):  # Paragraphs\n",
    "            para = element.text\n",
    "            if para.strip():\n",
    "                full_text.append(para.strip())\n",
    "        # elif element.tag.endswith(\"tbl\"):  # Tables\n",
    "        #     table_data = []\n",
    "        #     table = element\n",
    "        #     for row in table.rows:\n",
    "        #         row_data = [cell.text for cell in row.cells]\n",
    "        #         table_data.append(row_data)\n",
    "        #     tables.append(table_data)\n",
    "        elif element.tag.endswith(\"blip\"):  # Images\n",
    "            image_part = element.getparent().getparent().getparent()\n",
    "            image = image_part.blob  # Get the image blob\n",
    "\n",
    "            # Save image file\n",
    "            image_filename = os.path.join(output_dir, f\"image_{len(images)}.png\")\n",
    "            with open(image_filename, \"wb\") as img_file:\n",
    "                img_file.write(image)\n",
    "            images.append(image_filename)\n",
    "\n",
    "    return full_text, images, tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "docx_file = \"MBS PPT.docx\"  # Replace with your file path\n",
    "output_dir = \"ppt_assets\"  # Directory to save images\n",
    "extracted_text, images, tables = extract_content_from_docx(docx_file, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mortgage Backed Securities PrePayment Prediction using ML',\n",
       " 'Team CMT-6',\n",
       " 'Problem Statement',\n",
       " 'Valuing residential mortgage-backed securities (RMBS) involves complex considerations of macroeconomic factors and behavioral factors like prepayment and default.',\n",
       " 'Accurately predicting prepayment rates in residential RMBS is vital for investors and mortgage servicers.',\n",
       " 'Developing comprehensive prepayment prediction models that integrate macro and behavioral factors is essential.',\n",
       " 'Reliable prepayment predictions enable informed decision-making and effective risk management in the RMBS market.',\n",
       " 'Overview',\n",
       " \"A machine learning framework is proposed, utilizing Fannie Mae's loan-level dataset.\",\n",
       " 'The framework involves crucial steps: data preprocessing, feature engineering, model selection, and training.',\n",
       " 'Enhanced prediction accuracy achieved through this framework can lead to better risk management practices which ultimately contributes to maximizing profitability in the mortgage market.',\n",
       " 'Proposed Solution',\n",
       " 'Prediction of Single Monthly Mortality:',\n",
       " 'The SMM represents the rate at which the outstanding mortgage balances are prepaid each month and can be calculated as:',\n",
       " 'Prediction of Cash Inflows:',\n",
       " 'Cash inflows in mortgage-backed securities are payments from borrowers that provide income to investors. They come in the form of monthly principal and interest payments, crucial for evaluating MBS investment potential. It is calculated using the following formula.',\n",
       " 'Dataset',\n",
       " 'The Fannie Mae Single Family loan dataset contains comprehensive loan-level data on single-family mortgage loans backed by Fannie Mae.',\n",
       " 'The dataset spans from 2000 to 2020, with a random selection of approximately 15,000 records per quarter within the years 2000 to 2020, resulting in 1.2 million records.',\n",
       " 'Key data elements include loan identifiers, loan characteristics, borrower information, loan performance, property details, origination and servicing information.',\n",
       " 'Economic indicators, such as divorce rate , inflation rate and unemployment rate, were explicitly included as features.',\n",
       " 'The dataset was streamlined to 37 informative and relevant attributes for improved modeling and analysis efficiency.',\n",
       " 'Data Flow Diagram',\n",
       " 'Models used for Cash Inflow & SMM Calculation',\n",
       " 'Models used for Cash Inflow & SMM Calculation',\n",
       " 'Deployment Plan',\n",
       " 'Hosting platform like Heroku or cloud servers like AWS, Azure, etc. can be used for deployment.',\n",
       " 'After setting up runtime environment and configuring necessary libraries and packages, the code can be uploaded on the platform.',\n",
       " 'Due to the compliance policy this step has been omitted.',\n",
       " 'Future Scope',\n",
       " 'Larger Dataset: Utilize a larger dataset for training to get better results.',\n",
       " 'CPR PSA Calculation: Improve accuracy by adjusting prepayment projections for 12 months.',\n",
       " 'Predict Prepayment and Default: Enhance models to predict both prepayment and default behavior.',\n",
       " 'Scenario Analysis and Stress Testing: Incorporate scenario analysis and stress testing for risk assessment and contingency planning.',\n",
       " 'Thank You']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_summary(extracted_text):\n",
    "    openai.api_key = \"your_openai_api_key\"  # Replace with your OpenAI API key\n",
    "\n",
    "    prompt = (\n",
    "        f\"Extract topics and key points from the following document:\\n{extracted_text}\"\n",
    "    )\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\", prompt=prompt, max_tokens=1000, temperature=0.5\n",
    "    )\n",
    "\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Function to extract text, images, and tables from a Word document\n",
    "def extract_content_from_docx(docx_file, output_dir):\n",
    "    document = Document(docx_file)\n",
    "    full_text = []\n",
    "    images = []\n",
    "    tables = []\n",
    "\n",
    "    # Ensure the output directory exists for saving images\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Traverse through document elements\n",
    "    for element in document.element.body:\n",
    "        if element.tag.endswith(\"p\"):  # Paragraphs\n",
    "            para = element.text\n",
    "            if para.strip():\n",
    "                full_text.append(para.strip())\n",
    "        elif element.tag.endswith(\"tbl\"):  # Tables\n",
    "            table_data = []\n",
    "            table = element\n",
    "            for row in table.rows:\n",
    "                row_data = [cell.text for cell in row.cells]\n",
    "                table_data.append(row_data)\n",
    "            tables.append(table_data)\n",
    "        elif element.tag.endswith(\"blip\"):  # Images\n",
    "            image_part = element.getparent().getparent().getparent()\n",
    "            image = image_part.blob  # Get the image blob\n",
    "\n",
    "            # Save image file\n",
    "            image_filename = os.path.join(output_dir, f\"image_{len(images)}.png\")\n",
    "            with open(image_filename, \"wb\") as img_file:\n",
    "                img_file.write(image)\n",
    "            images.append(image_filename)\n",
    "\n",
    "    return full_text, images, tables\n",
    "\n",
    "\n",
    "# Function to query OpenAI's API for topic extraction and summarization\n",
    "def get_llm_summary(extracted_text):\n",
    "    openai.api_key = \"your_openai_api_key\"  # Replace with your OpenAI API key\n",
    "\n",
    "    prompt = (\n",
    "        f\"Extract topics and key points from the following document:\\n{extracted_text}\"\n",
    "    )\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\", prompt=prompt, max_tokens=1000, temperature=0.5\n",
    "    )\n",
    "\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "\n",
    "# Function to convert LLM response into JSON with text, images, and tables\n",
    "def convert_to_json(llm_output, images, tables):\n",
    "    topics = []\n",
    "    current_topic = \"\"\n",
    "    key_points = []\n",
    "\n",
    "    lines = llm_output.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        if line.strip() == \"\":\n",
    "            continue  # Skip empty lines\n",
    "\n",
    "        if \"Topic:\" in line or \"Topic\" in line:\n",
    "            if current_topic:\n",
    "                topics.append({\"topic\": current_topic, \"key_points\": key_points})\n",
    "            current_topic = line.replace(\"Topic:\", \"\").strip()\n",
    "            key_points = []\n",
    "        else:\n",
    "            key_points.append(line.strip())\n",
    "\n",
    "    if current_topic:\n",
    "        topics.append({\"topic\": current_topic, \"key_points\": key_points})\n",
    "\n",
    "    # Add images and tables to the JSON\n",
    "    json_output = {\n",
    "        \"topics\": topics,\n",
    "        \"images\": images,  # List of image file paths\n",
    "        \"tables\": tables,  # List of table data\n",
    "    }\n",
    "\n",
    "    return json_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document and extract text, images, and tables\n",
    "docx_file = \"your_document.docx\"  # Replace with your file path\n",
    "output_dir = \"output_images\"  # Directory to save images\n",
    "extracted_text, images, tables = extract_content_from_docx(docx_file, output_dir)\n",
    "\n",
    "# Get the summarized topics and key points from the LLM\n",
    "llm_output = get_llm_summary(\"\\n\".join(extracted_text))\n",
    "\n",
    "# Convert the LLM output to JSON format\n",
    "json_output = convert_to_json(llm_output, images, tables)\n",
    "\n",
    "# Save the JSON output to a file\n",
    "with open(\"output_with_images_tables.json\", \"w\") as json_file:\n",
    "    json.dump(json_output, json_file, indent=4)\n",
    "\n",
    "# Print the JSON data\n",
    "print(json.dumps(json_output, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shria\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shria\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shria\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shria\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shria\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shria\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['API_KEY'] = \"AIzaSyBfw1TqjNQjJwXYVqB9J34_Z-hBak0w-6Y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.environ[\"API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elara, a scrawny ginger cat with eyes the color of melted amber, stared at the tattered backpack perched precariously on the dusty attic floor. It was her grandmother's, passed down through generations, filled with whispers of forgotten adventures. Legend claimed it held a magic that could take you anywhere, but no one had dared open it for decades.\n",
      "\n",
      "Elara, however, wasn't afraid. Her life in the cramped, quiet attic was lonely. She yearned for the vibrant, bustling world beyond the dusty windowpane.  She remembered her grandmother's stories of soaring mountains, roaring seas, and cities sparkling with a thousand lights. Those stories fueled her desire, and tonight, she would open the backpack.\n",
      "\n",
      "Her paws fumbled with the worn leather straps, and a puff of dust erupted. Inside, nestled amongst moth-eaten clothes and a faded map, lay a shimmering silver compass. It hummed with a low, enticing hum. Elara gingerly picked it up.  The compass, held aloft, spun wildly, finally settling with its needle pointing directly at her.\n",
      "\n",
      "Suddenly, the attic floor trembled. Light spilled from the backpack, swirling and coalescing into a shimmering portal. Elara, her heart hammering, cautiously stepped through.\n",
      "\n",
      "She landed with a soft thud on emerald green grass, the air vibrant with the scent of wildflowers. A towering, snow-capped mountain loomed before her. This, Elara knew, was the mountain her grandmother had described, the source of the legendary Sunstone.\n",
      "\n",
      "The journey was arduous, fraught with perilous slopes and hidden crevasses. But with each step, Elara felt a renewed sense of purpose. The magic of the backpack hummed around her, granting her strength and agility she never knew she possessed.\n",
      "\n",
      "Reaching the summit, Elara found the Sunstone, glowing like a miniature sun in the heart of a glacier. As she touched it, the stone pulsed with warmth, and a voice, soft as the wind, whispered in her ear, \"You have chosen well, little one.  The Sunstone grants a wish.  Choose wisely.\"\n",
      "\n",
      "Elara, her eyes brimming with tears, wished for the world to see the magic in her heart, the courage she found within herself. The Sunstone, shimmering brighter than ever, pulsed with light, and a wave of warmth washed over Elara.\n",
      "\n",
      "She returned through the portal, the magic of the backpack fading, but the warmth in her heart remaining. Back in the attic, the compass lay silent, the portal gone. But Elara knew she wasn't the same. She had found her own adventure, and the magic of the backpack, though not tangible, would forever guide her journey. \n",
      "\n",
      "From then on, Elara, the once timid ginger cat, roamed the attic with a newfound confidence, her amber eyes sparkling with the reflection of a world she had tasted and a journey she would never forget. The magic of the backpack had given her more than she could ever imagine, and she knew, deep within her, that her own adventures were just beginning. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\"Write a story about a magic backpack.\")\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
